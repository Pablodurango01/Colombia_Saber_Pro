{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Instalaci\u00f3n limpia de dependencias compatibles\n",
    "!pip uninstall -y -q catboost numpy scipy scikit-learn\n",
    "!pip install -q numpy==1.24.4\n",
    "!pip install -q scipy==1.10.1\n",
    "!pip install -q scikit-learn==1.2.2\n",
    "!pip install -q catboost==1.2.3 optuna"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 0. Instalaci\u00f3n de librer\u00edas\n",
    "!pip install -q scikit-learn pandas numpy category_encoders lightgbm\n",
    "\n",
    "# %%\n",
    "# 1. Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import lightgbm as lgb\n",
    "from google.colab import drive\n",
    "\n",
    "# %%\n",
    "# 2. Montar Google Drive y rutas\n",
    "drive.mount('/content/drive')\n",
    "BASE_DIR = '/content/drive/MyDrive/ai_data'\n",
    "TRAIN_PATH = f\"{BASE_DIR}/train.csv\"\n",
    "TEST_PATH  = f\"{BASE_DIR}/test.csv\"\n",
    "\n",
    "# %%\n",
    "# 3. Carga de datos\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test  = pd.read_csv(TEST_PATH)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "# %%\n",
    "# 4. Preprocesamiento\n",
    "TARGET = 'RENDIMIENTO_GLOBAL'\n",
    "ID_COL = 'ID'\n",
    "\n",
    "def freq_encode(df, cols):\n",
    "    for c in cols:\n",
    "        freq = df[c].value_counts(normalize=True)\n",
    "        df[c] = df[c].map(freq)\n",
    "    return df\n",
    "\n",
    "# Unir train/test\n",
    "df_test[TARGET] = np.nan\n",
    "full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "cat_cols = full.select_dtypes(include=['object']).columns.drop([TARGET])\n",
    "num_cols = full.select_dtypes(include=[np.number]).columns.drop([ID_COL, TARGET], errors='ignore')\n",
    "\n",
    "# Encoding + Imputaci\u00f3n/Escalado\n",
    "full_enc = freq_encode(full.copy(), cat_cols)\n",
    "imp = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "num_scaled = scaler.fit_transform(imp.fit_transform(full_enc[num_cols]))\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "pca_feats = pca.fit_transform(num_scaled)\n",
    "print(f\"Componentes PCA: {pca.n_components_}\")\n",
    "\n",
    "# DataFrame procesado\n",
    "df_proc = pd.DataFrame(pca_feats, columns=[f\"pc{i+1}\" for i in range(pca.n_components_)])\n",
    "df_proc[cat_cols] = full_enc[cat_cols].reset_index(drop=True)\n",
    "meta = full[[ID_COL, TARGET]].reset_index(drop=True)\n",
    "df_proc = pd.concat([meta, df_proc], axis=1)\n",
    "\n",
    "# %%\n",
    "# 5. Separar train/test\n",
    "df_train2 = df_proc[df_proc[TARGET].notna()]\n",
    "df_test2  = df_proc[df_proc[TARGET].isna()].drop(columns=[TARGET])\n",
    "\n",
    "# Mapeo target\n",
    "y_map = {'bajo':0, 'medio-bajo':1, 'medio-alto':2, 'alto':3}\n",
    "inv_map = {v:k for k,v in y_map.items()}\n",
    "\n",
    "X = df_train2.drop(columns=[ID_COL, TARGET])\n",
    "y = df_train2[TARGET].map(y_map).astype(int)\n",
    "X_test = df_test2.drop(columns=[ID_COL])\n",
    "\n",
    "# %%\n",
    "# 6. Split para validaci\u00f3n local\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Imputaci\u00f3n SVM\n",
    "cols = X_tr.columns.astype(str)\n",
    "X_tr.columns = cols; X_val.columns = cols; X_test.columns = cols\n",
    "imp_s = SimpleImputer(strategy='median')\n",
    "X_tr = pd.DataFrame(imp_s.fit_transform(X_tr), columns=cols)\n",
    "X_val = pd.DataFrame(imp_s.transform(X_val), columns=cols)\n",
    "X_test = pd.DataFrame(imp_s.transform(X_test), columns=cols)\n",
    "\n",
    "# %%\n",
    "# 7. LightGBM GPU r\u00e1pido (sin errores de early_stop en wrapper)\n",
    "gbm_quick = lgb.LGBMClassifier(\n",
    "    objective='multiclass', num_class=4,\n",
    "    device='gpu', gpu_platform_id=0, gpu_device_id=0,\n",
    "    num_leaves=31, learning_rate=0.1, n_estimators=200, max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbm_quick.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "print(\"\u2705 LightGBM r\u00e1pido completado\")\n",
    "\n",
    "# %%\n",
    "# 8. Evaluaci\u00f3n local\n",
    "pred_val = gbm_quick.predict(X_val)\n",
    "print(\"Accuracy validaci\u00f3n:\", accuracy_score(y_val, pred_val))\n",
    "print(classification_report(y_val, pred_val, target_names=list(inv_map.values())))\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# 9. Entrenamiento final y submission final\n",
    "\n",
    "# Entrenamiento final sin early stopping (usar todo el set sin evaluaci\u00f3n interna)\n",
    "gbm_quick.fit(\n",
    "    pd.concat([X_tr, X_val]),\n",
    "    pd.concat([y_tr, y_val])\n",
    ")\n",
    "\n",
    "# Predicci\u00f3n y creaci\u00f3n de archivo de submission\n",
    "preds = gbm_quick.predict(X_test)\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: df_test2[ID_COL].values,\n",
    "    TARGET: [inv_map[int(p)] for p in preds]\n",
    "})\n",
    "submission_path = f\"{BASE_DIR}/submission_pca_lgbm_quick.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(\"\u2705 Submission creada en:\", submission_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJ75Mt_JgZOi",
    "outputId": "9c7ac43d-9d93-4e99-b818-15f196234c38"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Train shape: (692500, 21)\n",
      "Test shape: (296786, 20)\n",
      "Componentes PCA: 4\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 1328\n",
      "[LightGBM] [Info] Number of data points in the train set: 554000, number of used features: 18\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 13 dense feature groups (8.45 MB) transferred to GPU in 0.014582 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -1.387089\n",
      "[LightGBM] [Info] Start training from score -1.391216\n",
      "[LightGBM] [Info] Start training from score -1.395033\n",
      "[LightGBM] [Info] Start training from score -1.371993\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.2342\n",
      "[100]\tvalid_0's multi_logloss: 1.21807\n",
      "[150]\tvalid_0's multi_logloss: 1.21261\n",
      "[200]\tvalid_0's multi_logloss: 1.20942\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's multi_logloss: 1.20942\n",
      "\u2705 LightGBM r\u00e1pido completado\n",
      "Accuracy validaci\u00f3n: 0.4287581227436823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bajo       0.46      0.57      0.51     34597\n",
      "  medio-bajo       0.33      0.26      0.29     34455\n",
      "  medio-alto       0.32      0.25      0.28     34324\n",
      "        alto       0.54      0.63      0.58     35124\n",
      "\n",
      "    accuracy                           0.43    138500\n",
      "   macro avg       0.41      0.43      0.41    138500\n",
      "weighted avg       0.41      0.43      0.42    138500\n",
      "\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 1329\n",
      "[LightGBM] [Info] Number of data points in the train set: 692500, number of used features: 18\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 13 dense feature groups (10.57 MB) transferred to GPU in 0.016884 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -1.387092\n",
      "[LightGBM] [Info] Start training from score -1.391216\n",
      "[LightGBM] [Info] Start training from score -1.395031\n",
      "[LightGBM] [Info] Start training from score -1.371991\n",
      "\u2705 Submission creada en: /content/drive/MyDrive/ai_data/submission_pca_lgbm_quick.csv\n"
     ]
    }
   ]
  }
 ]
}
